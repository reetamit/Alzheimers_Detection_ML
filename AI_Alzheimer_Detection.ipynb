{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing\n",
        "### Subtask\n",
        "\n"
      ],
      "metadata": {
        "id": "nIg4yC6vz-7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Loading**: Load train and test data \"train.parquet\" and \"test.parquet\"\n",
        "\n",
        "* **Visualize**: It's good to visualize the class distribution before we apply balanced class propertions\n",
        "\n",
        "* **Stratification**: Performing a stratified train-validation split on the train_df DataFrame guarantees:\n",
        "\n",
        "    * Balanced class proportions across splits.\n",
        "\n",
        "    * More reliable validation metrics (especially for imbalanced datasets like medical imaging)."
      ],
      "metadata": {
        "id": "tN8T1Y9dBJIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "otekqddlzPDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "8224856c-d79b-4d0e-abbb-fdb97416b9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive_mounted = False\n",
        "if not drive_mounted:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    drive_mounted = True\n",
        "    print(\"Drive mounted.\")\n",
        "drive_path = \"/content/drive/MyDrive/Projects/HealthCare/AI_Alzheimer_Detection/data\"\n",
        "\n",
        "train_filepath = os.path.join(drive_path, \"train.parquet\")\n",
        "test_filepath = os.path.join(drive_path, \"test.parquet\")\n",
        "train_df = pd.read_parquet(train_filepath)\n",
        "test_df  = pd.read_parquet(test_filepath)\n",
        "\n",
        "def bytes_to_pixels(b: bytes) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert raw image bytes (e.g. JPEG/PNG) into a 2D numpy array of pixel values (grayscale).\n",
        "    \"\"\"\n",
        "    img = Image.open(io.BytesIO(b))  # convert to grayscale\n",
        "    return np.array(img)\n",
        "\n",
        "def extract_bytes(blob):\n",
        "    \"\"\"\n",
        "    Unwrap a dict‐wrapped binary payload if needed,\n",
        "    otherwise return blob directly.\n",
        "    \"\"\"\n",
        "    if isinstance(blob, dict):\n",
        "\n",
        "        for key in (\"bytes\", \"data\", \"image\"):\n",
        "            if key in blob and isinstance(blob[key], (bytes, bytearray)):\n",
        "                return blob[key]\n",
        "\n",
        "        for v in blob.values():\n",
        "            if isinstance(v, (bytes, bytearray)):\n",
        "                return v\n",
        "        raise TypeError(f\"No bytes found in dict payload: {list(blob.keys())}\")\n",
        "    return blob\n",
        "\n",
        "train_df[\"image\"] = train_df[\"image\"].apply(lambda blob: bytes_to_pixels(extract_bytes(blob)))\n",
        "test_df[\"image\"]  = test_df[\"image\"].apply(lambda blob: bytes_to_pixels(extract_bytes(blob)))\n",
        "\n",
        "#Below graph shows the class distribution in the training set has more number of samples from class/label 2\n",
        "train_df['label'].value_counts().plot(kind='bar')\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#Stratification\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation\n",
        "### Subtask"
      ],
      "metadata": {
        "id": "60aU9SDCEUFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfrmation**: Setup trasformation settings for both train and test dataset"
      ],
      "metadata": {
        "id": "jQJMPfkFEfCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Image Transformation Pipelines for MRI Classification\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # Randomly crops a region of the image and resizes it to 224x224.\n",
        "    # The crop size varies between 80% and 100% of the original image area.\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "    # Randomly flips the image horizontally with a 50% chance.\n",
        "    # Useful for symmetry-based augmentation in medical images.\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    # Randomly rotates the image within ±15 degrees.\n",
        "    # Helps the model become invariant to slight orientation changes.\n",
        "    transforms.RandomRotation(15),\n",
        "\n",
        "    # Randomly adjusts brightness and contrast by ±10%.\n",
        "    # Simulates lighting variations and scanner differences.\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "\n",
        "    # Converts the PIL image to a PyTorch tensor and scales pixel values to [0, 1].\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Normalizes the image using mean and std of 0.5 (for grayscale images).\n",
        "    # This centers pixel values around 0 with a range of [-1, 1].\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Testing/Validation Transformations (No Augmentation)\n",
        "# -----------------------------------------------\n",
        "# These are deterministic and ensure consistent evaluation.\n",
        "test_transform = transforms.Compose([\n",
        "    # Resizes the image to a fixed size of 224x224.\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    # Converts the image to a tensor.\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Applies the same normalization as the training set.\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n"
      ],
      "metadata": {
        "id": "gCoTL5B-2EIR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation & Clasification\n",
        "### Subtask"
      ],
      "metadata": {
        "id": "NERZUQv5E5fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Create Dataset**:Create PyTorch-compatible datasets from DataFrames\n",
        "\n",
        "* **MRI Clasification**: Weighted Sampling to Handle Class Imbalance\n",
        "\n",
        "* **Dataloader**: DataLoader Setup for Model Training"
      ],
      "metadata": {
        "id": "qU6aAZiwFAU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Custom PyTorch Dataset for MRI Images from DataFrame\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MRIDatasetFromDF(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_array = self.df.iloc[idx]['image']\n",
        "        label = int(self.df.iloc[idx]['label'])\n",
        "\n",
        "        # Convert NumPy array to grayscale PIL image\n",
        "        image = Image.fromarray(image_array.astype(np.uint8)).convert('L')\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Dataset Preparation for MRI Classification\n",
        "# -----------------------------------------------\n",
        "train_dataset = MRIDatasetFromDF(train_df, transform=train_transform)\n",
        "val_dataset   = MRIDatasetFromDF(val_df, transform=test_transform)\n",
        "test_dataset  = MRIDatasetFromDF(test_df, transform=test_transform)\n",
        "\n",
        "\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Count samples per class in training data\n",
        "class_counts = train_df['label'].value_counts().sort_index().values\n",
        "\n",
        "# Compute inverse frequency weights for each class\n",
        "class_weights = 1. / class_counts\n",
        "\n",
        "# Assign a weight to each sample based on its class\n",
        "sample_weights = [class_weights[label] for label in train_df['label']]\n",
        "\n",
        "# Create a sampler that draws samples with replacement,\n",
        "# favoring underrepresented classes to balance training batches\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Use weighted sampler for training to mitigate class imbalance\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "# Use standard loaders for validation and testing (no sampling)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "iYUFUuJA2IUX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Processing\n",
        "##Subtask\n"
      ],
      "metadata": {
        "id": "SUQq9_RW2QRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model settings**:\n",
        "Loss Function, Optimizer, and Learning Rate Scheduler\n",
        "\n",
        "**Training setup**: Load EfficientNet-B0 model for transfer learning and setting\n",
        "Class Weights for Imbalanced Data"
      ],
      "metadata": {
        "id": "LNvAUv8EF76t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Model Setup for Multi-Label MRI Classification with EfficientNet\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Select GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from torchvision.models import efficientnet_b0\n",
        "from torchvision.models import EfficientNet_B0_Weights\n",
        "model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "\n",
        "\n",
        "# Modify the first convolutional layer to accept 1-channel (grayscale) MRI images\n",
        "model.features[0][0] = nn.Conv2d(\n",
        "    in_channels=1, out_channels=32,\n",
        "    kernel_size=3, stride=2, padding=1, bias=False\n",
        ")\n",
        "num_classes = len(train_df['label'].unique())\n",
        "\n",
        "# Replace the final classification layer to match the number of output classes\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Compute inverse frequency weights for each class to handle imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_df['label']),\n",
        "    y=train_df['label'].values\n",
        ")\n",
        "\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Use BCEWithLogitsLoss for multi-label classification (assumes sigmoid activation on outputs)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Adam optimizer with default beta values and learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler that reduces LR by factor of 0.5 if validation loss plateaus for 3 epochs\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', patience=3, factor=0.5\n",
        ")\n"
      ],
      "metadata": {
        "id": "xMXffH262NL-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training & Validation\n",
        "###Subtask\n",
        "* Set training parameters\n",
        "\n",
        "* Training Phase loop (with Mixup)\n",
        "\n",
        "* Validate traning with Early Stopping Logic"
      ],
      "metadata": {
        "id": "1oS_PXYgHXwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Training Loop with Mixup Augmentation and Early Stopping\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import torch.nn.functional as F\n",
        "num_epochs = 30\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "patience_limit = 5  # Stop training if no improvement for 5 consecutive epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass to get output shape for one-hot encoding\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Apply Mixup augmentation\n",
        "        lam = np.random.beta(0.4, 0.4)  # Sample mixing coefficient from Beta distribution\n",
        "        index = torch.randperm(images.size(0))  # Shuffle indices for mixing\n",
        "\n",
        "        # Mix images and one-hot encoded labels\n",
        "        mixed_images = lam * images + (1 - lam) * images[index]\n",
        "        num_classes = outputs.size(1)\n",
        "        labels_onehot = F.one_hot(labels, num_classes=num_classes).float()\n",
        "        labels_mixed = lam * labels_onehot + (1 - lam) * labels_onehot[index]\n",
        "\n",
        "        # Forward pass on mixed images\n",
        "        outputs = model(mixed_images)\n",
        "\n",
        "        # Compute loss using mixed labels\n",
        "        loss = criterion(outputs, labels_mixed)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation Phase\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Convert labels to one-hot encoding for BCEWithLogitsLoss\n",
        "            labels_onehot = F.one_hot(labels, num_classes=num_classes).float()\n",
        "            loss = criterion(outputs, labels_onehot)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy using argmax over softmax probabilities\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            predicted = torch.argmax(probs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    # Adjust learning rate if validation loss plateaus\n",
        "    scheduler.step(avg_val_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Early Stopping Logic\n",
        "    # -------------------------------\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0  # Reset counter on improvement\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience_limit:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "apyFiPrN2iNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save model\n",
        "Save the model's learned parameters (weights and biases) to a file.\n",
        "This stores only the state_dict (not the full model architecture), which can be reloaded later for inference or fine-tuning."
      ],
      "metadata": {
        "id": "VCZuMB_3IE_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_filepath = os.path.join(drive_path, \"AI_Alzmrmdl_enetb0.pth\")\n",
        "torch.save(model.state_dict(), model_filepath)\n"
      ],
      "metadata": {
        "id": "7o39A0gM2oTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Model file"
      ],
      "metadata": {
        "id": "UBLrHqb82_fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.efficientnet_b0(weights=None)\n",
        "activations = []\n",
        "gradients = []\n",
        "\n",
        "model.features[0][0] = nn.Conv2d(\n",
        "    in_channels=1,\n",
        "    out_channels=32,\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    padding=1,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(model_filepath, map_location=device))\n"
      ],
      "metadata": {
        "id": "265lMdGa3BBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "###\n",
        "  Plot SHAP overlay"
      ],
      "metadata": {
        "id": "ArWBjNEGI3gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------\n",
        "# Model Evaluation: SHAP (SHapley Additive exPlanations) for Model Interpretability\n",
        "#-----------------------------------------------------------------------------------\n",
        "\n",
        "import shap\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Patch SiLU globally to avoid in-place ops on views\n",
        "class SafeSiLU(nn.SiLU):\n",
        "    def forward(self, input):\n",
        "        return F.silu(input.clone(), inplace=self.inplace)\n",
        "\n",
        "def patch_silu(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.SiLU):\n",
        "            parent = model\n",
        "            parts = name.split('.')\n",
        "            for part in parts[:-1]:\n",
        "                parent = getattr(parent, part)\n",
        "            setattr(parent, parts[-1], SafeSiLU(inplace=module.inplace))\n",
        "\n",
        "# Load EfficientNet-B0 and modify for grayscale input and 4-class output\n",
        "model = models.efficientnet_b0(weights=None)\n",
        "model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(model_filepath, map_location=\"cpu\"))\n",
        "\n",
        "# Patch SiLU and set eval mode\n",
        "patch_silu(model)\n",
        "model.eval().to(device)\n",
        "\n",
        "# Wrap model to avoid view-related gradient errors\n",
        "class SafeModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x.clone())\n",
        "\n",
        "safe_model = SafeModel(model)\n",
        "\n",
        "# Prepare background data for SHAP\n",
        "background = next(iter(train_loader))[0][:100].to(device)\n",
        "\n",
        "# Initialize SHAP explainer with safe model\n",
        "explainer = shap.GradientExplainer(safe_model, background)\n",
        "\n",
        "# Select test samples\n",
        "test_images, _ = next(iter(test_loader))\n",
        "# Select a single test image\n",
        "test_image = test_images[0].unsqueeze(0)  # shape: (1, 1, 224, 224)\n",
        "\n",
        "# Compute SHAP values for the single image\n",
        "shap_values = explainer.shap_values(test_image)\n",
        "\n",
        "# Select SHAP values for class 0\n",
        "shap_values_class0 = shap_values[0][0]  # shape: (224, 224)\n",
        "\n",
        "# Convert grayscale image to RGB format\n",
        "image = test_image[0].cpu().numpy().squeeze(0)  # shape: (224, 224)\n",
        "image_rgb = np.stack([image]*3, axis=-1)        # shape: (224, 224, 3)\n",
        "\n",
        "# Normalize to [0, 1] range for imshow\n",
        "image_rgb = (image_rgb - image_rgb.min()) / (image_rgb.max() - image_rgb.min())\n",
        "\n",
        "# Reshape to batch format for SHAP\n",
        "image_rgb_batch = np.expand_dims(image_rgb, axis=0)  # shape: (1, 224, 224, 3)\n",
        "shap_values_batch = [np.expand_dims(shap_values_class0, axis=0)]  # shape: (1, 224, 224)\n",
        "\n",
        "# Plot SHAP overlay\n",
        "shap.image_plot(shap_values_batch, image_rgb_batch)\n"
      ],
      "metadata": {
        "id": "c0FN01cO3Jz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "###Subtask\n",
        "  * Display or analyze the confidence scores\n",
        "\n",
        "  * Plot entropy per sample"
      ],
      "metadata": {
        "id": "vFba2ewMJal6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Model Evaluation: Displaying Confidence Scores for One Batch\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Set the model to evaluation mode.\n",
        "# This disables dropout and batch normalization updates for consistent inference.\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient computation to reduce memory usage and speed up inference.\n",
        "import torch.nn.functional as F\n",
        "sample_confidences = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        # Move to CPU and convert to NumPy\n",
        "        batch_confidences = probs.cpu().numpy()\n",
        "\n",
        "        # Append to list\n",
        "        sample_confidences.extend(batch_confidences)\n",
        "\n",
        "        # Stop after collecting 20 samples\n",
        "        if len(sample_confidences) >= 10:\n",
        "            sample_confidences = sample_confidences[:10]\n",
        "            break\n",
        "\n",
        "for i, conf in enumerate(sample_confidences):\n",
        "    true_label = true_labels[i]\n",
        "    print(f\"Sample {i+1} | True Label: {true_label} | Confidences: {conf}\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Assuming sample_confidences is a list of softmax outputs (shape: [num_samples, num_classes])\n",
        "entropies = [entropy(probs, base=2) for probs in sample_confidences]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(range(1, len(entropies) + 1), entropies, color='orange')\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Entropy (bits)\")\n",
        "plt.title(\"Prediction Uncertainty per Sample\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_P8MXMgT3O8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "###Plot the confusion matrix as a heatmap"
      ],
      "metadata": {
        "id": "-rqMr6_NJvHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Model Evaluation: Confusion Matrix Visualization\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # For evaluation metrics\n",
        "import seaborn as sns  # For heatmap visualization\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "\n",
        "# Set the model to evaluation mode to disable dropout and batch norm updates\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store all predicted and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Disable gradient calculation for faster inference and lower memory usage\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the appropriate device (CPU or GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass through the model to get raw output scores (logits)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get the predicted class by selecting the index with the highest score\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Store predictions and true labels for later evaluation\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print overall test accuracy\n",
        "accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Print detailed classification metrics: precision, recall, f1-score, and support per class\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Compute the confusion matrix to visualize prediction errors\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # Annotate with counts, use blue color map\n",
        "plt.title(\"Confusion Matrix\")  # Title of the plot\n",
        "plt.xlabel(\"Predicted\")  # Label for x-axis\n",
        "plt.ylabel(\"True\")  # Label for y-axis\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "fSWxp96v3Rr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "###Grad-CAM Visualization"
      ],
      "metadata": {
        "id": "N4tdlo0jJ3vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Model Evaluation: Grad-CAM Visualization for Model Interpretability\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "# Select one sample image for each class label (0 to 3) from the validation dataframe\n",
        "samples_by_label = [val_df[val_df['label'] == i].iloc[0] for i in range(4)]\n",
        "\n",
        "# Import Grad-CAM utilities for visualization\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# Import plotting and image processing libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Create a 1-row, 4-column subplot layout for displaying Grad-CAM visualizations\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "# Loop through each selected sample (one per class)\n",
        "for i, sample in enumerate(samples_by_label):\n",
        "    # Extract image data and label from the sample\n",
        "    image_array = sample['image']\n",
        "    label = int(sample['label'])\n",
        "\n",
        "    # Convert image array to grayscale PIL image, then apply test transform and move to device\n",
        "    image = Image.fromarray(image_array.astype(np.uint8)).convert('L')\n",
        "    input_tensor = test_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize Grad-CAM with the model and target layer\n",
        "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "    # Specify the target class for which CAM should be generated\n",
        "    targets = [ClassifierOutputTarget(label)]\n",
        "\n",
        "    # Generate the grayscale CAM for the input tensor\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
        "\n",
        "    # Prepare the RGB version of the original image for overlay\n",
        "    rgb_image = np.stack([image_array / 255.0] * 3, axis=-1)\n",
        "\n",
        "    # Resize the CAM to match the original image dimensions\n",
        "    grayscale_cam_resized = cv2.resize(grayscale_cam, (rgb_image.shape[1], rgb_image.shape[0]))\n",
        "\n",
        "    # Overlay the CAM on the original image\n",
        "    visualization = show_cam_on_image(rgb_image, grayscale_cam_resized, use_rgb=True)\n",
        "\n",
        "    # Display the visualization in the subplot\n",
        "    axs[i].imshow(visualization)\n",
        "    axs[i].set_title(f\"Grad-CAM for label {label}\")\n",
        "    axs[i].axis('off')  # Hide axis ticks for cleaner display\n"
      ],
      "metadata": {
        "id": "2eU27wvt3UrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to test Single Image\n",
        "  * Final function to predict Alzheimer's disease stage from a single MRI image.\n",
        "  * Includes Grad-CAM visualization to highlight regions influencing the model's decision.\n",
        "  * This implementation can be extended for batch processing or integration into a web service."
      ],
      "metadata": {
        "id": "iiO6TGm03aUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.efficientnet_b0(weights=None)\n",
        "activations = []\n",
        "gradients = []\n",
        "\n",
        "model.features[0][0] = nn.Conv2d(\n",
        "    in_channels=1,\n",
        "    out_channels=32,\n",
        "    kernel_size=3,\n",
        "    stride=2,\n",
        "    padding=1,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "def show_gradcam(image_path, cam):\n",
        "    image = Image.open(image_path).convert('L').resize((224, 224))\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    plt.imshow(image_np, cmap='gray')\n",
        "    plt.imshow(cam, cmap='jet', alpha=0.5)\n",
        "    plt.title(\"Grad-CAM Overlay\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def predict_with_gradcam(image_path):\n",
        "\n",
        "    image = Image.open(image_path).convert('L')\n",
        "    input_tensor = test_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    activations.clear()\n",
        "    gradients.clear()\n",
        "\n",
        "    output = model(input_tensor)\n",
        "    pred_class = output.argmax(dim=1)\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    output[0, pred_class].backward()\n",
        "\n",
        "    # Get gradients and activations\n",
        "    grad = gradients[0].cpu().detach()\n",
        "    act = activations[0].cpu().detach()\n",
        "\n",
        "    # Compute Grad-CAM\n",
        "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
        "    cam = (weights * act).sum(dim=1).squeeze()\n",
        "    cam = torch.relu(cam)\n",
        "\n",
        "    # Normalize and resize\n",
        "    cam -= cam.min()\n",
        "    cam /= cam.max()\n",
        "    cam = transforms.Resize((224, 224))(cam.unsqueeze(0)).squeeze()\n",
        "\n",
        "    return pred_class.item(), cam.numpy()\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    activations.append(output)\n",
        "\n",
        "def backward_hook(module, grad_input, grad_output):\n",
        "    gradients.append(grad_output[0])\n",
        "\n",
        "target_layer = model.features[-1][0]\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)\n",
        "sample_iput_image = os.path.join(drive_path, \"MRI_image1.png\")\n",
        "\n",
        "#al_stage = predict_single_image(image_path)\n",
        "#print(f\"Predicted Alzheimer stage lebel: {al_stage} \")\n",
        "\n",
        "label, cam = predict_with_gradcam(sample_iput_image)\n",
        "print(f\"Predicted Alzheimer stage label: {label} \")\n",
        "show_gradcam(sample_iput_image, cam)\n"
      ],
      "metadata": {
        "id": "3x30qZNG3XML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}